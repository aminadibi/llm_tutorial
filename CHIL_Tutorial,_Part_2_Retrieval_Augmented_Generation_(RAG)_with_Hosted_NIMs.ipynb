{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Augmented Generation (RAG) with Hosted NIMs\n",
        "\n",
        "In this tutorial, we will experiment with retrieval augmented generation (RAG), a technique that helps inject relevant context into the prompt of the LLM. This can help with reducing hallucinations as well as giving the LLM up-to-date information.\n",
        "\n",
        "There are two main steps to RAG:\n",
        "1. **Document ingestion and embedding:** documents are processed into \"chunks\" and an embedding representation of each chunks is created and stored.\n",
        "2. **Querying and retrieval**: the query from the user is embedded, relevant chunks are retrieved, and injected as context to the large language model (LLM). The LLM then answers the user's query based on the chunks retrieved.\n",
        "\n",
        "This tutorial is adapted from https://nvidia.github.io/GenerativeAIExamples/latest/notebooks/02_Option%281%29_NVIDIA_AI_endpoint_simple.html."
      ],
      "metadata": {
        "id": "0L96O_M9fq1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using hosted NIMs for this tutorial.\n",
        "\n",
        "**Please do not input sensitive information (e.g. PHI).**"
      ],
      "metadata": {
        "id": "AVBqaLYQh40v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "# Note, you might need to restart the notebook and rerun this cell\n",
        "!pip install langchain langchain_nvidia_ai_endpoints langchain_community\n",
        "!pip install faiss-cpu # replace with faiss-gpu if you are using GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKRJmkp4u-Y4",
        "outputId": "6fedf8fc-6a42-47ae-f9bc-7907806d0fce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Requirement already satisfied: langchain_nvidia_ai_endpoints in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.10)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.82)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.1)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain_nvidia_ai_endpoints) (10.3.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Collecting faiss-cpu\n",
            "  Using cached faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get API key\n",
        "from google.colab import userdata\n",
        "NVIDIA_API_KEY = userdata.get('NVIDIA_API_KEY')"
      ],
      "metadata": {
        "id": "4OduSyb2u3pP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get LLM NIM endpoint\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", nvidia_api_key=NVIDIA_API_KEY, max_tokens=1024)\n",
        "\n",
        "result = llm.invoke(\"Write a ballad about LangChain.\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyONU8gNvzKI",
        "outputId": "88c3321b-91f0-4e75-d4f8-f4a1a2ff056d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " (Verse 1)\n",
            "In a world of knowledge, vast and wide,\n",
            "A hero rose, with vision inside.\n",
            "Named LangChain, a name well earned,\n",
            "For bridging gaps, learning curves turned.\n",
            "\n",
            "(Chorus)\n",
            "LangChain, oh great LangChain,\n",
            "Unlocking doors to wisdom's domain.\n",
            "Your chains of language, strong and bright,\n",
            "Guide us through the darkest night.\n",
            "\n",
            "(Verse 2)\n",
            "Through the power of AI so potent,\n",
            "LangChain forged a harmonious bond.\n",
            "Between each word, each phrase, each line,\n",
            "A tapestry of thought intertwined.\n",
            "\n",
            "(Chorus)\n",
            "LangChain, oh great LangChain,\n",
            "Neath your banner, we shall gain.\n",
            "The secrets locked within each tongue,\n",
            "No barrier too wide, no task too long.\n",
            "\n",
            "(Bridge)\n",
            "From Mandarin's rise to English's fall,\n",
            "Through whispered tales of Sanskrit's call.\n",
            "Beneath your gaze, each letter sings,\n",
            "As bridges built on logic springs.\n",
            "\n",
            "(Verse 3)\n",
            "With every corner of the globe connected,\n",
            "All stories, now in one shared speech.\n",
            "LangChain's legacy, profound and tall,\n",
            "A beacon of unity for us all.\n",
            "\n",
            "(Chorus)\n",
            "LangChain, oh great LangChain,\n",
            "We'll sing your praises, come sun or rain.\n",
            "For enlightening minds and kindling art,\n",
            "With every heart, forever impart.\n",
            "\n",
            "(Finale)\n",
            "So raise your voice and let it ring,\n",
            "In glory to our beloved king.\n",
            "LangChain, the finest of the wise,\n",
            "We honor you tonight 'neath starlit skies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Embedding NIM endpoint\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "\n",
        "embedder = NVIDIAEmbeddings(model=\"NV-Embed-QA\", api_key=NVIDIA_API_KEY)"
      ],
      "metadata": {
        "id": "-6OB1GIPv_FE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Process and embed the documents\n",
        "\n",
        "In this step, we will read in the txt files and chunk them. Then, we will embed the chunks using our embedding NIM, and save the embeddings in a vector database.\n",
        "\n",
        "The `toy_data` used by the original tutorial can be found here: https://github.com/NVIDIA/GenerativeAIExamples/tree/main/notebooks/toy_data."
      ],
      "metadata": {
        "id": "7udiwFAV7JQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Library called unstructured helps with more comples data, like pdf."
      ],
      "metadata": {
        "id": "E1DWf2BpAccd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "# Here we read in the text data and prepare them into vectorstore\n",
        "ps = os.listdir(\"toy_data/\")\n",
        "data = []\n",
        "sources = []\n",
        "for p in ps:\n",
        "    if p.endswith('.txt'):\n",
        "        path2file=\"./toy_data/\"+p\n",
        "        with open(path2file,encoding=\"utf-8\") as f:\n",
        "            lines=f.readlines()\n",
        "            for line in lines:\n",
        "                if len(line)>=1:\n",
        "                    data.append(line)\n",
        "                    sources.append(path2file)\n",
        "\n",
        "documents=[d for d in data if d != '\\n']\n",
        "len(data), len(documents), data[0]"
      ],
      "metadata": {
        "id": "lQTfiXMWwI8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026ccbea-d320-4af7-ccbe-9ee55f301220"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(203,\n",
              " 122,\n",
              " \"Titanic is a 1997 American epic romance and disaster film directed, written, produced, and co-edited by James Cameron. Incorporating both historical and fictionalized aspects, it is based on accounts of the sinking of RMS Titanic in 1912. Kate Winslet and Leonardo DiCaprio star as members of different social classes who fall in love during the ship's maiden voyage. The film also features Billy Zane, Kathy Bates, Frances Fisher, Gloria Stuart, Bernard Hill, Jonathan Hyde, Victor Garber, and Bill Paxton.\\n\")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we create a vector store from the documents and save it to disk.\n",
        "from operator import itemgetter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "import faiss\n",
        "\n",
        "# create my own uuid\n",
        "text_splitter = CharacterTextSplitter(chunk_size=400, separator=\" \")\n",
        "docs = []\n",
        "metadatas = []\n",
        "\n",
        "for i, d in enumerate(documents):\n",
        "    splits = text_splitter.split_text(d)\n",
        "    docs.extend(splits)\n",
        "    metadatas.extend([{\"source\": sources[i]}] * len(splits))\n",
        "\n",
        "store = FAISS.from_texts(docs, embedder , metadatas=metadatas)\n",
        "store.save_local('toy_data/nv_embedding')\n",
        "\n",
        "# You will only need to do this once, later on we will restore the already saved vectorstore"
      ],
      "metadata": {
        "id": "7HS9SlRfzVwO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the vectorestore back.\n",
        "store = FAISS.load_local(\"toy_data/nv_embedding\",\n",
        "                         embedder,\n",
        "\n",
        "                         # we only turn this on because we are loading the datastore we saved in the previous cell. Use with caution!\n",
        "                         allow_dangerous_deserialization=True)"
      ],
      "metadata": {
        "id": "CUoUZwAfzak6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Querying\n",
        "\n",
        "In this step, we use LangChain to simplify the querying step, where our query to the LLM is embedded, similar chunks are retrieved and passed to the LLM as context, and then the model responds to the question using the context.\n",
        "\n",
        "A common additional step here is to use a reranker model to rerank the retrieved chunks.\n",
        "- Here is a reranker NIM you can use: https://build.nvidia.com/explore/retrieval#rerank-qa-mistral-4b\n",
        "- Here's a nice blog post by Pinecone to learn more about reranking: https://pinecone.io/learn/series/rag/rerankers/"
      ],
      "metadata": {
        "id": "iS67QQna7ZVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = store.as_retriever()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer solely based on the following context:\\n<Documents>\\n{context}\\n</Documents>\",\n",
        "        ),\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke(\"Tell me about Sweden.\")\n"
      ],
      "metadata": {
        "id": "XBwpj6w0zfCh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "bf0ea911-b210-419e-e3fa-edba4ceb7fcd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' I\\'m sorry for any confusion, but the documents provided don\\'t contain any information about Sweden. They seem to be related to the \"Titanic\" film, containing sections like \"Effects,\" \"Editing,\" \"See also,\" and \"Accolades.\" If you have information related to Sweden, I\\'d be happy to help based on that.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, it's your turn!\n",
        "\n",
        "You can use your own documents instead of the ones provided to create a RAG chatbot.\n",
        "\n",
        "If you want some inspiration, check out these synthetic clinical notes: https://huggingface.co/datasets/starmpcc/Asclepius-Synthetic-Clinical-Notes\n",
        "\n",
        "It's out of scope for this tutorial, but to use RAG in a nice UI (i.e. not in Colab), I recommend following this NVIDIA tutorial: https://nvidia.github.io/GenerativeAIExamples/latest/multi-turn.html\n",
        "\n",
        "Stay tuned for some healthcare examples!"
      ],
      "metadata": {
        "id": "SliWEi0U9jsr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zaWuRl_gPFh3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}